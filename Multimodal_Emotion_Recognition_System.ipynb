{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "SdgYu1CRP2_1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate, Dropout, Conv2D, MaxPooling2D, Flatten, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import cv2  # For video processing\n",
        "\n",
        "# Define paths for video data\n",
        "TRAIN_VIDEO_PATH = '/kaggle/input/PES-ml-hack-link1/train_videos'\n",
        "TEST_VIDEO_PATH = '/kaggle/input/PES-ml-hack-link1/test_videos'\n",
        "\n",
        "# Load the data\n",
        "train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/train.csv', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/test.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Function to extract video features with CNN\n",
        "def extract_video_features(video_path):\n",
        "    try:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "\n",
        "        # Extract up to 10 frames from each video\n",
        "        while len(frames) < 10 and cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.resize(frame, (64, 64))  # Resize to 64x64 for CNN\n",
        "            frames.append(frame)\n",
        "        cap.release()\n",
        "\n",
        "        # Pad frames if fewer than 10\n",
        "        if len(frames) < 10:\n",
        "            frames.extend([np.zeros_like(frames[0])] * (10 - len(frames)))\n",
        "\n",
        "        # Return array of shape (10, 64, 64, 3)\n",
        "        return np.array(frames[:10])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "        return np.zeros((10, 64, 64, 3))\n",
        "\n",
        "# Process video data for training and testing\n",
        "def load_video_features(df, video_dir):\n",
        "    video_features = []\n",
        "    for _, row in df.iterrows():\n",
        "        video_file = f\"dia{row['Dialogue_ID']}_utt{row['Utterance_ID']}.mp4\"\n",
        "        video_path = os.path.join(video_dir, video_file)\n",
        "        video_features.append(extract_video_features(video_path))\n",
        "    return np.array(video_features)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['Emotion'] = label_encoder.fit_transform(train_df['Emotion'])\n",
        "y = to_categorical(train_df['Emotion'])\n",
        "\n",
        "# Text preprocessing with Keras Tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df['Utterance'])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_df['Utterance'])\n",
        "train_padded = pad_sequences(train_sequences, maxlen=100, padding='post')\n",
        "\n",
        "# Split data for validation\n",
        "train_text, val_text, y_train, y_val = train_test_split(train_df, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load video features for train and validation sets\n",
        "X_train_text = pad_sequences(tokenizer.texts_to_sequences(train_text['Utterance']), maxlen=100, padding='post')\n",
        "X_val_text = pad_sequences(tokenizer.texts_to_sequences(val_text['Utterance']), maxlen=100, padding='post')\n",
        "X_train_video = load_video_features(train_text, TRAIN_VIDEO_PATH)\n",
        "X_val_video = load_video_features(val_text, TRAIN_VIDEO_PATH)\n",
        "\n",
        "# Define a text model\n",
        "input_text = Input(shape=(100,), name='text_input')\n",
        "x = Embedding(input_dim=10000, output_dim=128)(input_text)  # Increased embedding size\n",
        "x = LSTM(128, return_sequences=True)(x)\n",
        "x = LSTM(64)(x)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# Define a CNN-LSTM model for video features\n",
        "input_video = Input(shape=(10, 64, 64, 3), name='video_input')\n",
        "v = TimeDistributed(Conv2D(32, (3, 3), activation='relu'))(input_video)\n",
        "v = TimeDistributed(MaxPooling2D((2, 2)))(v)\n",
        "v = TimeDistributed(Flatten())(v)\n",
        "v = LSTM(64)(v)  # LSTM over time-distributed CNN features\n",
        "v = Dropout(0.5)(v)\n",
        "\n",
        "# Concatenate text and video models\n",
        "combined = Concatenate()([x, v])\n",
        "output = Dense(5, activation='softmax')(combined)  # Assuming 5 emotion classes\n",
        "\n",
        "# Create and compile the model\n",
        "model = Model(inputs=[input_text, input_video], outputs=output)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit([X_train_text, X_train_video], y_train,\n",
        "                    epochs=15, batch_size=32,\n",
        "                    validation_data=([X_val_text, X_val_video], y_val))\n",
        "\n",
        "# Evaluate the model on the test data (if available)\n",
        "# Uncomment and adjust the following lines if you have test data\n",
        "# test_features_video = load_video_features(test_df, TEST_VIDEO_PATH)\n",
        "# X_test_text = pad_sequences(tokenizer.texts_to_sequences(test_df['Utterance']), maxlen=100, padding='post')\n",
        "# y_test = to_categorical(label_encoder.transform(test_df['Emotion']))\n",
        "# test_loss, test_accuracy = model.evaluate([X_test_text, test_features_video], y_test)\n",
        "# print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Save the model\n",
        "model.save('multimodal_emotion_model.h5')\n",
        "\n",
        "# Generate predictions on the test set (if applicable)\n",
        "# predictions = model.predict([X_test_text, test_features_video])\n",
        "# predicted_classes = np.argmax(predictions, axis=1)\n",
        "# predicted_emotions = label_encoder.inverse_transform(predicted_classes)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "# results_df = pd.DataFrame({'Utterance_ID': test_df['Utterance_ID'], 'Predicted_Emotion': predicted_emotions})\n",
        "# results_df.to_csv('emotion_predictions.csv', index=False)\n",
        "# print(\"Predictions saved to 'emotion_predictions.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/train_videos.zip"
      ],
      "metadata": {
        "id": "4tWa1Nd8amSF",
        "outputId": "d5684caf-5fe0-45fd-e827-7393366d1671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/train_videos.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /content/train_videos.zip or\n",
            "        /content/train_videos.zip.zip, and cannot find /content/train_videos.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "KZNTm9PXP2_5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "ca31839b-158a-4f3d-9403-61ae64ffabb0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/PES-ml-hack-link1/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2298c280e7e7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/PES-ml-hack-link1/train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1252'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Define path to video clips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvideo_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/kaggle/input/PES-ml-hack-link2/train_videos'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/PES-ml-hack-link1/train.csv'"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/train.csv', encoding='1252')\n",
        "# Define path to video clips\n",
        "video_dir = '/kaggle/input/PES-ml-hack-link2/train_videos'\n",
        "\n",
        "\n",
        "# Function to get video file path from IDs\n",
        "def get_video_clip_path(row):\n",
        "    dialogue_id = row['Dialogue_ID']\n",
        "    utterance_id = row['Utterance_ID']\n",
        "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "    return os.path.join(video_dir, filename)\n",
        "\n",
        "# Apply the function to get file paths for each sampled clip\n",
        "train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n",
        "\n",
        "# Check sample paths\n",
        "print(train_df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test code :\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Load train data\n",
        "train_df = pd.read_csv('/mnt/data/train (1).csv')\n",
        "video_dir = '/kaggle/input/PES-ml-hack-link2/train_videos'\n",
        "\n",
        "# Get video clip paths\n",
        "def get_video_clip_path(row):\n",
        "    dialogue_id = row['Dialogue_ID']\n",
        "    utterance_id = row['Utterance_ID']\n",
        "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "    return os.path.join(video_dir, filename)\n",
        "\n",
        "train_df['video_clip_path'] = train_df.apply(get_video_clip_path, axis=1)\n",
        "\n",
        "# Text Feature Extraction with BERT\n",
        "class TextFeatureExtractor(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        super(TextFeatureExtractor, self).__init__()\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "    \n",
        "    def forward(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=50)\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Video Feature Extraction with ResNet\n",
        "class VideoFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VideoFeatureExtractor, self).__init__()\n",
        "        self.model = models.resnet18(pretrained=True)\n",
        "        self.model.fc = nn.Identity()  # Remove final layer to get feature vector\n",
        "    \n",
        "    def forward(self, video_path):\n",
        "        # Video preprocessing\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = Image.fromarray(frame)\n",
        "            frame = transforms.ToTensor()(frame)\n",
        "            frames.append(frame)\n",
        "            if len(frames) == 16:  # Sample 16 frames per video\n",
        "                break\n",
        "        cap.release()\n",
        "        \n",
        "        # Stack and pass through model\n",
        "        frames = torch.stack(frames)\n",
        "        with torch.no_grad():\n",
        "            features = self.model(frames)\n",
        "        return features.mean(dim=0)  # Average features across frames\n",
        "\n",
        "# Custom Dataset\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df, text_extractor, video_extractor):\n",
        "        self.df = df\n",
        "        self.text_extractor = text_extractor\n",
        "        self.video_extractor = video_extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        text = row['Text']\n",
        "        video_path = row['video_clip_path']\n",
        "        \n",
        "        # Extract text and video features\n",
        "        text_features = self.text_extractor(text)\n",
        "        video_features = self.video_extractor(video_path)\n",
        "        \n",
        "        # Combine features\n",
        "        features = torch.cat((text_features, video_features), dim=1)\n",
        "        label = torch.tensor(row['Emotion_Label'])  # Assuming 'Emotion_Label' is in the CSV\n",
        "        \n",
        "        return features, label\n",
        "\n",
        "# Define the multimodal model\n",
        "class MultimodalFusionModel(nn.Module):\n",
        "    def __init__(self, text_dim, video_dim, num_classes):\n",
        "        super(MultimodalFusionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(text_dim + video_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "    \n",
        "    def forward(self, text_features, video_features):\n",
        "        features = torch.cat((text_features, video_features), dim=1)\n",
        "        x = torch.relu(self.fc1(features))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize components\n",
        "text_extractor = TextFeatureExtractor()\n",
        "video_extractor = VideoFeatureExtractor()\n",
        "dataset = MultimodalDataset(train_df, text_extractor, video_extractor)\n",
        "\n",
        "# Training loop\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "model = MultimodalFusionModel(text_dim=768, video_dim=512, num_classes=4)  # Adjust dimensions as needed\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "for epoch in range(10):  # Example epochs\n",
        "    for features, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8L1zp0dWbi5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "cZouIzPsP2_7"
      },
      "outputs": [],
      "source": [
        "train_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "ue2bWZB8P2_7"
      },
      "outputs": [],
      "source": [
        "# Define path to video clips\n",
        "train_df = pd.read_csv('/kaggle/input/PES-ml-hack-link1/test.csv', encoding='1252')\n",
        "video_dir = '/kaggle/input/PES-ml-hack-link2/test_videos'\n",
        "\n",
        "\n",
        "# Function to get video file path from IDs\n",
        "def get_video_clip_path(row):\n",
        "    dialogue_id = row['Dialogue_ID']\n",
        "    utterance_id = row['Utterance_ID']\n",
        "    filename = f\"dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
        "    return os.path.join(video_dir, filename)\n",
        "\n",
        "# Apply the function to get file paths for each sampled clip\n",
        "df['video_clip_path'] = df.apply(get_video_clip_path, axis=1)\n",
        "\n",
        "# Check sample paths\n",
        "print(df[['Dialogue_ID', 'Utterance_ID', 'video_clip_path']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "iNog61cRP2_8"
      },
      "outputs": [],
      "source": [
        "all_preds = [\"your_prediction\" for i in df['Utterance_ID']]\n",
        "all_ids = df[\"Sr No.\"]\n",
        "submission_df = pd.DataFrame({\n",
        "        'Sr No.': all_ids,\n",
        "        'Emotion': all_preds\n",
        "    })\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 10107338,
          "sourceId": 88291,
          "sourceType": "competition"
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}